%% Next line for the spell checker to disable warnings about $f(x)^2$ vs $f{(x)}^2$.
% chktex-file asdfqweasdfqwe3  
%%%
 
\documentclass[10pt, a4paper]{article}
%\documentclass[10pt, a4paper, openany, draft]{book}
    

%%%%%%%%%%%%%%%%%%%%
%%% To relabel equations with AUCTeX use the command
%%%   M-x reftex-renumber-simple-labels 


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%% Standard packages
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% amsmath,amssymb,    

%\usepackage{graphicx}% Include figure files   
%\usepackage{dcolumn}% Align table columns on decimal point
 
%\usepackage[mathlines]{lineno}% Enable numbering of text and display math
%\linenumbers\relax % Commence numbering lines
 
\usepackage{enumerate}%i), ii) etc...

\usepackage[utf8]{inputenc}
\usepackage[T1]{fontenc}
%\usepackage{mathptmx}

\usepackage{amsmath,amssymb}
\usepackage{mathtools}  
\usepackage{amsthm} %% For \newtheorem{}{}
%\usepackage{mathrsfs} %% \mathscr ;; conflict with bbm
%\usepackage{stix} %% 
\usepackage{bbm} %% \mathbbm to get the blackboard bold 1;; conflict with mathrsfs
\usepackage{stmaryrd} %% to get [[ and ]] 

\usepackage{hyperref}

\usepackage{slashed}


\usepackage{soul} %% \ul for underline which wraps at the end of line

\usepackage{tikz}
\usepackage{tikz-cd} %% commutative diagrams
\usetikzlibrary{arrows}
\tikzcdset{arrow style=tikz, diagrams={>=stealth}} %%smaller arrow heads
\usepackage{graphicx}  

\usetikzlibrary{shapes.geometric,quotes}


\usepackage{simplewick}
%\usepackage{wick}

%\usepackage{genyoungtabtikz}



\usepackage{etoolbox}

%\usepackage{dsfont} %double stroke character alternative to \mathbb 
%\usepackage{amssymb}  %% For \mathbb %%% package is redundant when using 'stix' package
%\usepackage{bbold} %% must be placed *after* amssymb to get the nice \mathbb{1}
%\usepackage{bm}% bold math
%\usepackage{stix2}
%\usepackage{boondox-cal}
%\usepackage[cal=boondox]{mathalfa}
%\usepackage{pzccal}
%\usepackage{frcursive}
%%%%
%%%%%%%%[Standard packages]


%\usepackage[title]{appendix}
\usepackage{appendix}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%  Standard environments
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\theoremstyle{plain}
\newtheorem{theorem}{Theorem}[section]
\newtheorem*{theorem*}{Theorem} 
\newtheorem{proposition}[theorem]{Proposition}
\newtheorem{lemma}[theorem]{Lemma}
\newtheorem{corollary}{Corollary}[theorem]

\theoremstyle{definition}
\newtheorem{definition}[theorem]{Definition}
\theoremstyle{definition}
\newtheorem*{definitions}{Definitions}
\theoremstyle{definition}
\newtheorem*{definition*}{Definition}

%\numberwithin{equation}{definition}

\theoremstyle{definition}
\newtheorem{remark}[theorem]{Remark}
\theoremstyle{definition}
\newtheorem*{remark*}{Remark}
\theoremstyle{definition}
\newtheorem{remarks}[theorem]{Remarks}
\theoremstyle{definition}
\newtheorem*{remarks*}{Remarks}

\theoremstyle{remark}
\newtheorem{claim}[theorem]{Claim}
%\theoremstyle{remark}
%\newtheorem*{claim*}[theorem]{Claim}
\theoremstyle{remark}
\newtheorem{idea}[theorem]{Idea}
\newtheorem{example}[theorem]{Example}
\newtheorem*{example*}{Example}
\newtheorem*{examples*}{Examples}

%%%  -------------------------------------------------------------------
%%%  Define a "rudin-style-paragraph" (:-D)
%%%  -------------------------------------------------------------------
%%%
%%% This is the better version which actually defines a *new* theorem style instead
%%% of re-defining the existing one...
\newtheoremstyle{rudin-style-generic}
	{}% <Space above>
	{}% <Space below>
	{}% <Body font>
	{}% <Indent amount>
	{\bfseries}% <Head font>
	{}% <Punctuation head>	
	{ }% <Space after head>
	% {\thmnumber{\S\hspace{2pt}#2} \thmname{#1.}\thmnote{\phantom{,}#3 }}% <Theorem head spec>
        {\thmnumber{#2} \thmname{#1.}\thmnote{\phantom{,}#3 }}% <Theorem head spec>
\newtheoremstyle{rudin-style-generic*}{}{}{}{}{\bfseries}{}{ }{\thmname{#1}\thmnote{#3}}
\theoremstyle{rudin-style-generic}
\newtheorem{parpar}{}[section]
\theoremstyle{rudin-style-generic*}
\newtheorem{parpar*}{}[]{}
%\AtEndEnvironment{parpar*}{\phantom{a}\null\hfill\S}%  %%% requires \usepackage{etoolbox}
%\AtEndEnvironment{parpar}{\null\hfill\S}%   %%% requires \usepackage{etoolbox}
\newenvironment{parpar-noproof}
  {%\renewcommand{\qedsymbol}{$\heartsuit$}%
   \pushQED{\qed}\begin{parpar}}
  {\popQED\end{parpar}}

%%% "Rudin" theorem-style with bodyfont = itshape
\newtheoremstyle{rudin-style-theorem}{}{}{\itshape}{}{\bfseries}{}{ }{\thmnumber{\S\hspace{2pt}#2} \thmname{#1.}\thmnote{\phantom{,}#3}}
\theoremstyle{rudin-style-theorem}
\newtheorem{parpar-theorem}[parpar]{Theorem}
\newtheorem{parpar-proposition}[parpar]{Proposition}
\newtheorem{parpar-lemma}[parpar]{Lemma}
\newtheorem{parpar-corollary}[parpar]{Corollary}

%%% "Rudin" proof-environment with wider left margin 
\newenvironment{parpar-proof}[1][\scshape    %%%\shshape=small cap font
\proofname]
{%
	\proof[#1]%
	\setlength{\leftskip}{3em}%
}
{%
	\endproof%
      }
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%% New \ref command for paragraphs \begin{parpar}....\end{parpar}
%%%% ==============================================================
%%%% It needs to a custom .cwl file if one wants autocompletion as for \ref
%%%% For texstudio see:
%%%% https://tex.stackexchange.com/questions/414281/configuring-auto-completion-with-texstudio
%%%%
\newcommand{\parref}[1]{\S\textbf{\ref{#1}}}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\newcommand{\more}[1]{{{\noindent{}\hspace{-2cm}\color{red}{{$\blacktriangleright[[${{#1}}$]]$}}}}}


%%%---------------------------------------------------------------------
%%%  -------------------------------------------------------------------
%%%  Proofs, QEDsymbol, etc, small paragraphs... 
%%%  -------------------------------------------------------------------
%%%
%%% Extract Yinyang from the package marvosym to use it as qed symbol :^)
%\newcommand{\YinYangQED}{{\usefont{U}{mvs}{m}{n}\symbol{89}}}
%%% Make the Yin Yang the qed symbol
%\renewcommand\qedsymbol{\YinYangQED}

%%%% Change the qed box at the end of proof (older version 2)
\newenvironment{funproof}[1][\scshape\proofname]
{%
	%\proof[#1]%
	\renewcommand*\qedsymbol{‌​$\square$ (claim)}%
	%\renewcommand*\qedsymbol{‌​$\YinYangQED$ (claim)}
}
{%
	\endproof%
}

%\newcommand*\shortProof{(\textit{Proof.})}
\newenvironment{shortproof}{{\small\scshape\proofname}.}{\hfill\footnotesize\qedsymbol}    
%% to use inside enumerate or itemize environments
%% the proof environment from amsthm would add too much white space
%%% small proof: a proof in small font
%% codied from
%% http://tex.stackexchange.com/questions/148955/how-to-change-the-font-size-of-all-theorem-environments
\newenvironment{smallproof}
	{%
	\vspace{.5em}
	\setlength\parindent{0pt}%
	\pushQED{\qed}%
	\textsc{\footnotesize\proofname}.
	\setlength{\leftskip}{3em}%
	}
	{%
		\endproof\footnotesize\qedhere\popQED%\hfill\footnotesize\qedsymbol%
	}
\AtBeginEnvironment{smallproof}{\footnotesize}   %%requires \usepackage{etoolbox}

\newenvironment{smallerparagraph}{}{}
\AtBeginEnvironment{smallerparagraph}{\small}    %%requires \usepackage{etoolbox}


%%% environment with custom font size
%% codied from
%% http://tex.stackexchange.com/questions/4139/how-to-change-font-size-mid-document
\newenvironment{computation}[1]
{%
	\clearpage
	\let\orignewcommand\newcommand
	\let\newcommand\renewcommand
	\makeatletter
	%\input{bk#1.clo}%% only for book class
	\input{size10.clo}%
	\makeatother
	\let\newcommand\orignewcommand
}
{%
	\clearpage
}

%%%%
%%%%%%%%[Standard environments]



%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%% Standard notation
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\usepackage{mathtools}
\newcommand\SmallMatrix[1]{{%
  \tiny\arraycolsep=0.3\arraycolsep\ensuremath{\begin{pmatrix}#1\end{pmatrix}}}}

\newcommand{\todo}[1]{{\color{red}\textit{[TO-DO#1]}}}


\newcommand{\deq}{\stackrel{\mathrm{def}}{=}}
\newcommand{\dd}{\,\mathrm{d}}
\newcommand{\ii}{\mathrm{i}}
%\usepackage{bbm}
\newcommand{\id}{\,\mathbbm{1}}
\DeclareMathOperator*{\Dom}{Dom}
\DeclareMathOperator*{\Ran}{Range}
\newcommand{\PP}{{\mathbb{P}}}
\newcommand{\QQ}{{\mathbb{Q}}}
\newcommand{\KK}{{\mathbb{K}}}


\newcommand{\NN}{\mathbb N}
\newcommand{\ZZ}{\mathbb Z}
\newcommand{\RR}{\mathbb R}
\newcommand{\CC}{\mathbb C}

\newcommand{\Gg}{{\mathbf{G}}}
\newcommand{\Hg}{{\mathbf{H}}}
\newcommand{\Ng}{{\mathbf{N}}}
\newcommand{\Sg}{{\mathbf{S}}}


\newcommand{\HH}{\mathbb H}
\newcommand{\TT}{\mathbb T} %% torus
\newcommand{\Mat}{\mathrm M} %% space of matrices
\DeclareMathOperator{\tr}{Tr}
\DeclareMathOperator{\Span}{Span}
\DeclareMathOperator{\idfunction}{id}

%\newcommand{\st}{\left|\vphantom{\tfrac{1}{2}}\right.}
\renewcommand{\st}{\,\mathbf:\,}
%\newcommand{\st}{\,\mathbf:\,}
\newcommand{\ST}{\,\mathbf:\,}
\newcommand{\setset}[2]{\left\{#1\;  \mathbf: \; #2  \right\}}




%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%% permanent, hafnian, pfaffian
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\DeclareMathOperator*{\pf}{pf}
\DeclareMathOperator*{\haf}{haf}
\DeclareMathOperator*{\per}{per}

%%% sign (of a permutation for example)
\DeclareMathOperator*{\sgn}{sgn}

%% Tensor algebras
\newcommand*{\TAlg}[1]{\Gamma_{\!\otimes}#1}
\newcommand*{\SymTAlg}[1]{\Gamma_{\!\odot}#1}
\newcommand*{\AntiTAlg}[1]{\Gamma_{\!\wedge}#1}
%% Bosonic Fock space:
%\newcommand*\BFock[1]{\bigodot\!#1} 
\newcommand*\BFock[1]{\mathbb\Gamma_{\!\odot}#1} 
%% Fermionic Fock space:
%\newcommand*\FFock[1]{\bigwedge\!#1} 
\newcommand*\FFock[1]{\mathbb\Gamma_{\!\wedge}#1}
%% General Fock space:
%\newcommand*\GFock[1]{\bigotimes\!#1} %{\mathscr{F}_{\otimes}}
\newcommand*\GFock[1]{\mathbb\Gamma_{\!\otimes}#1} %{\mathscr{F}_{\otimes}}
%% Finite number of particles - Scwartz wave functions - Bosonic Fock space
\newcommand*\FSBFock{{\mathscr{F}_{s}^{0}}} 
%% Deprecated
\newcommand*{\FockNew}{{%
		\text{\fontencoding{OT2}\selectfont\char3}% %\char3,\char5,\char88,\char16,\char17, I
}}


%%% (LIE) ALGEBRA %%%
\newcommand*{\Cliff}{\mathcal C\ell}
\newcommand*{\CCliff}{\CC\ell}
\newcommand*{\SO}{\mathbf{SO}}
\newcommand*{\SU}{\mathbf{SU}}
\newcommand*{\UU}{\mathbf{U}}
\newcommand*{\Spin}{\mathbf{Spin}}
\newcommand*{\Pin}{\mathbf{Pin}}
\newcommand*{\SL}{\mathbf{SL}}
\newcommand*{\SLL}{{\mathbf{SL}(2,\CC)}}
\newcommand*{\SLLR}{{{\mathbf{SL}(2,\CC)}_\RR}}
\newcommand*{\iSLLR}{{{\mathbf{ISL}(2,\CC)}_\RR}}

\newcommand*{\GL}{\mathbf{GL}}
\newcommand*{\Sp}{\mathbf{Sp}}
\newcommand*{\PoiUC}{\widetilde{\mathbf{\Pi}}}
\newcommand*{\ISpin}{\mathbf{ISpin}}
\newcommand*{\IPin}{\mathbf{IPin}}
\newcommand*{\ISO}{\mathbf{ISO}}
\newcommand*{\ad}{\mathfrak{a}\;\!\!\mathfrak{d}}
\newcommand*{\Ad}{\text{Ad}\,}
\newcommand*{\End}{\text{End}\,}
\newcommand*{\Aut}{\text{Aut}\,}
\newcommand*{\Diff}{\text{Diff}\,}
\newcommand*{\Gau}{\text{Gau}\,}

\newcommand*{\Oup}{{\mathscr{O}_{\mathring{v}}^\uparrow}} %% upper hyperboloid
\newcommand*{\dOup}{{\dd_{\mathring v}^\uparrow}} %% measure on upper hyperboloid

% \newcommand*{\rest}{\!\!\upharpoonright} %% restriction symbol
\newcommand*{\rest}{{\upharpoonright}} %% restriction symbol

\newcommand*{\ElleUpDown}{L}
\newcommand*{\ElleUp}{{L^\Up}}
\newcommand*{\ElleDown}{L^\Down}
\newcommand*{\elleUpDown}{l}
\newcommand*{\elleUp}{{l^\Up}}
\newcommand*{\elleDown}{{l^\Down}}
\newcommand*{\poff}{\mathpzc{p}}
\newcommand*{\MC}{{\mathrm{M}_{\mathtt{C}}}}
\newcommand*{\dotprod}{{\boldsymbol{\cdot}}}
\newcommand*{\scalareta}[2]{{\langle{} {#1},{#2}{\rangle}_{\!\eta}}}
\newcommand*{\scalardelta}[2]{{\langle{} {#1},{#2}{\rangle}_{\!\delta}}}


\newcommand*{\pUp}{{p^\Up}}
\newcommand*{\pUpDown}{{p}}
\newcommand*{\pDown}{{p^\Down}}
\newcommand*{\pST}{{\mathcal p}}

\newcommand*{\fin}{{f^{\text{in}}}}
\newcommand*{\SET}[1]{{\{ 0, \dots, #1 - 1 \}}}
\newcommand*{\boldcol}{{\,\boldsymbol{:}\,}} 
\DeclareFontEncoding{LS1}{}{}
\DeclareFontSubstitution{LS1}{stix}{m}{n}
\DeclareSymbolFont{symbols2}{LS1}{stixfrak}{m}{n}
\DeclareMathSymbol{\typecolon}{\mathbin}{symbols2}{"25}


% completed direct sum and product
\DeclareMathOperator*{\Hoplus}{\widehat\oplus}
\DeclareMathOperator*{\Hbigoplus}{\widehat{\bigoplus}}
\DeclareMathOperator*{\Hotimes}{\widehat\otimes}
\DeclareMathOperator*{\Hsymtimes}{\widehat\odot}
\DeclareMathOperator*{\Hantitimes}{\widehat\wedge}
\DeclareMathOperator*{\Hwedge}{\hat{\wedge}}

%%% PROBABILITY %%%
\newcommand{\Expect}{\mathbb{E}}
\newcommand{\Var}{\mathrm{Var}}
\newcommand{\Cov}{\mathrm{Cov}}




%%% exterior algebra symbol (similar to a big \Lambda)
%%% taken from: http://tex.stackexchange.com/questions/61637/wedge-power-symbol
\makeatletter
\newcommand{\extp}{\@ifnextchar^\@extp{\@extp^{\,}}}
\def\@extp^#1{\mathop{\bigwedge\nolimits^{\!#1}}}
\makeatother
%%% -- edit -- now it seams to work.. (??:^)
%%% the previous commented section would be better but causes unexpected problems.. ;(
%%% so I make do with the following
%\newcommand*{\Extern}{\displaystyle\bigwedge\!}
%%% --- edit --- since now the sofisticated method above seams to work I now define the following
\newcommand*{\Extern}{{\extp}}


%\usepackage{accents}
%\newcommand{\ubar}[1]{\underaccent{\bar}{{#1}}}

\usepackage{stackengine}

\newcommand\ubar[1]{\stackunder[1.2pt]{$#1$}{\rule{.8ex}{.075ex}}}
%%%%
%%%%%%%%[Standard notation]


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%% Notation for  Dirac Rosetta stone
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%


\usepackage{MnSymbol} %% I need \ostar
%\newcommand{\ostar}{{{\textcircled{$\star$}}}}
%\usepackage{wasysym}
%\newcommand{\ostar}{{\APLlog}}
%\newcommand{\ostar}{{\circledast}}


% \newcommand{\PPpar}{{\mathbb P_{\mathrm{par}}}}
%\newcommand{\PPant}{{\mathbb P_{\mathrm{ant}}}}
\newcommand{\Par}{{\mathrm{par}}}
\newcommand{\Ant}{{\mathrm{ant}}}
\newcommand{\FW}{{\mathrm{FW}}}
\newcommand{\ParRed}{\mathrm{PAR}}
\newcommand{\AntRed}{\mathrm{ANT}}

\newcommand{\tildegamma}{{\tilde{\gamma}}}


\newcommand{\Even}{{\mathrm{even}}}
\newcommand{\Odd}{{\mathrm{odd}}}

\newcommand{\Orbit}{{\mathscr{O}_m^{\uparrow}}}

\newcommand{\Vone}{{W}}
\newcommand{\Vtwo}{{V}}
\newcommand{\TZSpace}{{\mathfrak{H}}} %% time-zero space
\newcommand{\TZDom}{{\mathscr S(\RR^3)\otimes\Vtwo}} %% time-zero domain
\newcommand{\CovSpace}{{\mathcal{H}}} %% covariant space
\newcommand{\DiracSpace}{{\mathcal{H}_{\mathrm{D}}}}
\newcommand{\PASpace}{{\mathcal{V}}} %% particle-antiparticle space
\newcommand{\pa}{{\mathcal{v}}} %% particle-antiparticle wave function
\newcommand{\PNSpace}{{\mathcal{V}'}} %% positive-negative energy space
\newcommand{\pn}{{\mathcal{v}'}} %% positive-negative energy wave function


\newcommand{\Orb}{{\mathscr{O}}} %% Orbit for  Spin = Spin^0 U {\gamma_5}
\newcommand{\OrbUp}{{\mathscr{O}_{m^2}^{\uparrow}}} %% Orbit for  Spin^0, p_0>0
\newcommand{\OrbDown}{{\mathscr{O}_{m^2}^{\downarrow}}} %% Orbit for  Spin^0, p_0<0
\newcommand{\OrbUpDown}{{\mathscr{O}_{m^2}^{\uparrow} \cup \mathscr{O}_{m^2}^{\downarrow}}} 

\newcommand{\Fpa}{{\mathcal{F}_{(1)}}} %% Fock 1-(particle+antiparticle) space (Wigner representation for both par and antipar)
%\newcommand{\Hpa}{{\mathcal{H}_{\mathbf{p}}}}  %% Complex-twisted particle+antiparticle space (Charge conj. anti-unitary)
\newcommand{\VSpin}{{\mathcal{V}}} %% Carrier space for rep of \RR x Spin = \RR x (Spin^0 U {\gamma_5})
\newcommand{\Hpn}{{\mathcal{H}_{\updownarrow}}} %% (Positive-negative energy)-twisted space space
\newcommand{\HD}{{\mathcal{H}_{D}}} %% Dirac space (covariant space in momentum representation)
\newcommand{\WSpinzero}{{\mathcal{H}_{\uparrow}}} %% Standard Wigner rep space for particle & antiparticles


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%% SPACES
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\newcommand{\Hgeneric}{{\mathcal{W}}}
\newcommand{\fgeneric}[1][]{{w_{#1}}}
\newcommand{\fgenericslashed}[1][]{{\slashed{w}_{#1}}}

\newcommand{\Virr}{{W}}
\newcommand{\Vred}{{V}}

\newcommand{\wigner}{{\uparrow}}
\newcommand{\Hwigner}{{\mathcal{W}_\wigner}}
\newcommand{\fwigner}[1][]{{w_{#1}^\wigner}}


\newcommand{\parant}{{\uparrow\oplus\uparrow}}
\newcommand{\Hpa}{{\mathcal{W}_\parant}}
\newcommand{\HpaExtended}{{\mathcal{V}_\parant}}
\newcommand{\HpaExtendedCovariant}{{ {\vphantom{|}\mathcal{V}}^{\ElleUp}_{\parant} }}
\newcommand{\fpa}[1][]{{\mathcal{v}_{#1}}}
\newcommand{\fpaext}[1][]{{v_{#1}^\parant}}






\newcommand{\FF}{{\uparrow\oplus\downarrow}} %%FF: field-function 
\newcommand{\Hff}{\mathcal{K}_\FF} %% Wigner field-function space 
\newcommand{\HffCovariant}{{ {\vphantom{|}\mathcal{K}}^{\ElleUpDown}_{\FF} }} %% Covariant field-function space
\newcommand{\HffExtended}{{\mathcal{H}_\FF}} %% Extended Wigner field-function space 
\newcommand{\HffExtendedCovariant}{{ {\vphantom{|}\mathcal{H}}^{\ElleUpDown}_{\FF} }}
\newcommand{\fff}[1][]{{f_{#1}^\FF}}
\newcommand{\fffext}[1][]{{f_{#1}^\FF}}
% \newcommand{\Hcov}[1][]{{{\HffExtendedCovariant}}} %%
\newcommand{\Hcov}[1][]{{{\mathcal{H}^{\textrm{cov}}_{#1}}}} %%
\newcommand{\HPhi}{{\mathcal{H}_{\Phi}}} %%
\newcommand{\HPi}{{\mathcal{H}_{\Pi}}} %%

\newcommand{\fcov}[1][]{{{f_{#1}}}} %% argument of the fields
\newcommand{\fST}[1][]{{{\varphi}}} %% space time test function
\newcommand{\fzero}[1][]{{{F_{#1}}}} %% time-zero function


%%%% ALIASES FOR SPACES:
%%%% irr, red  ;   pa, ff  ;  wigner, cov
%%%%
%%%% \Hpa
%%%% \HPA    \HPAcov
%%%% \HFF    \HFFcov
%%%% \Hff    \Hffcov
%%%%
\newcommand{\HPA}{\HpaExtended}
\newcommand{\HFF}{\HffExtended}
\newcommand{\HPAcov}{\HpaExtendedCovariant}
\newcommand{\HFFcov}{\HffExtendedCovariant}
\newcommand{\Hffcov}{\HffCovariant}

%%%% FIELDS
\newcommand{\fieldKG}{{\phi}}
\newcommand{\fieldIrr}{{\psi}}
\newcommand{\fieldCoord}{{\Phi}}
\newcommand{\fieldMoment}{{\Pi}}

\newcommand{\STfieldKG}{{\phi}}
\newcommand{\STfieldReduced}{{\psi}}
\newcommand{\STfieldCoord}[1][{0}]{{\Phi_{#1}}}
\newcommand{\STfieldMoment}[1][{0}]{{\Pi_{#1}}}

\let\div\relax
\DeclareMathOperator{\div}{{\mathrm{div}}}


%%% OLD

\newcommand{\Hextended}{{\mathcal{V}_\Up}}
\newcommand{\fextended}[1][]{{v_{#1}^\Up}}
\newcommand{\HextendedDown}{{\mathcal{V}_\Down}}



\newcommand{\total}{{\uparrow\oplus\downarrow}}
\newcommand{\Htotal}{{\mathcal{H}_\total}}
\newcommand{\ftotal}{{f^\total}}
\newcommand{\ptotal}[1][]{{p_{#1}^\total}}

\newcommand{\reduced}{{\updownarrow}}
\newcommand{\Hreduced}{{\mathcal{H}_\reduced}}
\newcommand{\freduced}{{f^\reduced}}

\newcommand{\cov}{{\mathrm{cov}}}
\newcommand{\Htotalcov}{{\mathcal{H}_\cov}}  %{{\mathcal{H}_\total^L}}
\newcommand{\ftotalcov}[1][]{{{f_{#1}^\cov}}}

\newcommand{\Hcoord}{{\mathcal{H}_\cov^\phi}}
\newcommand{\fcoord}[1][]{{f_{#1}^\phi}}
\newcommand{\Hmoment}{{\mathcal{H}_\cov^\pi}}
\newcommand{\fmoment}[1][]{{f_{#1}^\pi}}

\newcommand{\redcov}{}
\newcommand{\Hredcov}{{\mathcal{K}_\redcov}}
\newcommand{\fredcov}{{f^\reduced}}


\newcommand{\Wightman}[1][]{{\mathrm{W}_{\!{#1}}}}
\newcommand{\Wightmandual}{{\tilde{\mathrm{W}}}}
\newcommand{\Wfunction}{{\mathcal{w}}}
\newcommand{\Wfunctiondual}{{\tilde{\mathscr{W}}}}
\newcommand{\Sfunction}{{\mathcal{S}}}




\def\ccar#1#2{{{[ {#1} , {#2} ]}_\ostar}}
\newcommand{\piUp}{{\varpi_\Up}}
\newcommand{\piDown}{{\varpi_\Down}}
\newcommand{\piUpDown}{{\varpi_{\Up\cup\Down}}}
\newcommand{\restUpDown}{{{\rest_{\Up\cup\Down}}}}

\DeclareMathOperator*{\Inv}{{\mathrm{Inv}}}


\newcommand{\BoseFermiFock}[1][\!]{{\Gamma_{\!\ostar}{#1}}}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%





\newcommand{\dirac}{{D}}
\newcommand{\Hdirac}{{\mathcal{H}_\dirac}}
\newcommand{\fdirac}{{f_\dirac}}

\newcommand{\Hparticle}{{\mathcal{W}_\uparrow}}
\newcommand{\fparticle}{{w_\uparrow}}
\newcommand{\Hfield}{{\mathcal{H}}}
\newcommand{\ffield}{{f}}
\newcommand{\Sfield}{{\mathcal{S}}}

\newcommand{\HwignerRed}{{\mathcal{V_\Up}}} %%% Wigner space for ``reducible particle''
\newcommand{\HwignerRedDown}{{\mathcal{V_\Down}}} %%% Wigner space for ``reducible particle''
\newcommand{\Hwignertotal}{{\mathcal{U}_\Up}} %%% Total space of reducible PAR & ANT
\newcommand{\Hwignertotalcov}{{\mathcal{U}_\Up^{\ElleUp}}} %%% Total space of reducible PAR & ANT

%\newcommand{\cov}{{\mathrm{cov}}}
\newcommand{\Hwignercov}{{\tilde{\Hwigner}}}
\newcommand{\fwignercov}{{\tilde{\fwigner}}}




\newcommand{\Up}{{\uparrow}}
\newcommand{\Down}{{\downarrow}}
\newcommand{\pcircUp}{{\mathring{k}^\Up}}
\newcommand{\pcircDown}{{\mathring{k}^\Down}}
\newcommand{\pcirc}{{\mathring{k}}}

% \DeclareMathOperator*{\SpinEta}{{\Spin(\varepsilon(\eta)\eta)}}
% \DeclareMathOperator*{\SpinEtaZero}{{\Spin^0(\varepsilon(\eta)\eta)}}
\DeclareMathOperator*{\SpinEta}{{\Spin}}
\DeclareMathOperator*{\SpinEtazero}{{\Spin^0}}

% \newcommand{\FF}{{\mathbb{V}}} %% Fermi Fock space
% \newcommand{\FFfin}{{\mathbb{V}_{\textrm{fin}}}} %% Fermi Fock space
% \newcommand{\ff}{{\mathbbm{v}}} %% element of the Fermi Fock space
% \newcommand{\vac}{{\mathrm{Vac}}} %% vacuum state


\usepackage{tikz}
\usetikzlibrary{matrix,arrows.meta}
\usepackage{amsmath}
\DeclareMathOperator{\im}{im}


%%%%%%%%%%%%%%%%%%%
%%% For book document class:
%\usepackage{times,lipsum}
\usepackage[margin=1in]{geometry}
% \usepackage[onehalfspacing]{setspace}
\usepackage[]{setspace}

\newcommand{\chapterabstract}[1]{
  \begin{quote}
    \singlespacing\small
    \rule{14cm}{1pt}\\
    {#1}
    \vskip-4mm
    \rule{14cm}{1pt}
  \end{quote}}
%%%%%%%%%%%%%%%%%% 


%%%%
%%%%%%%%[Notation Dirac Rosetta stone]

%%%% taken from
%%%% https://tex.stackexchange.com/questions/83509/hfill-in-math-mode
\makeatletter
\newcommand{\pushright}[1]{\ifmeasuring@#1\else\omit\hfill$\displaystyle#1$\fi\ignorespaces}
\newcommand{\pushleft}[1]{\ifmeasuring@#1\else\omit$\displaystyle#1$\hfill\fi\ignorespaces}
\makeatother




\title{Deep learning, notes}
\author{L. Borasi}
 
\begin{document}
\maketitle
\begin{abstract}
	Personal notes while reading the book~\cite{goodfellowDeepLearning2016}.
\end{abstract}
\tableofcontents


\section{MLP}
\subsection{General notation}\label{sec:general-notation}
Let the output of a NN be a vector $f_{\text{out}}\in \mathcal H_{\text{out}}$ 
where $\mathcal H_{\text{out}}$ is a Hilbert space with scalar product $\langle\cdot,\cdot\rangle$
and Hilbert norm $\|f\|\deq \sqrt{\langle f,f\rangle}$.
 
Consider $(e_i)_{i\in I}$ to be a Hilbert basis of $\mathcal H_{\text{out}}$.
Then
\begin{equation*}
	\| f_{\text{out}} \|^2 = \sum_{i\in I} \langle f_{\text{out}}, e_i \rangle^2 = \sum_{i\in I}  [f_{\text{out}}]_i^2 
	\quad v\in\mathcal H_{\text{out}}
	,
\end{equation*}
where we have used the notation $[f]_i \deq \langle f, e_i\rangle$, $i\in I$, to denote the $i$-th component of $f$.

Similarly let $\mathcal H_{\text{in}}$ the vector space of inputs.
Moreover, let $\mathbb V$ be the vector space of weights.
Following the standard convention we consider a weight $V\in\mathbb V$ to be composed of two components:
$V = (b,w)$ where $b\in \mathbb B$, $w\in\mathbb W$, with $\mathbb V=\mathbb B\oplus\mathbb V$.
An element $W\in\mathbb W$ is a collection of weighted edges which characterize the NN.
An element $B\in\mathbb B$ is the collection of ``additive coefficients'' (see below).

We now consider the \textit{transition function}  $F:\mathcal H_{\text{in}} \times \mathbb V \rightarrow \mathcal H_{\text{out}}$.
This function characterizes the NN completely, in the sense that, given the weights $w\in\mathcal W$ and the inputs $f_{\text{in}}$,
$F(f_{\text{in}}, w)\in\mathcal H_{\text{out}}$ is the output returned by the NN.

We consider the \textit{cost function} $S:\mathbb V\rightarrow \RR$ given by
\begin{equation*}
	S(V) \deq \sum_{f_{\text{in}}\in\mathcal H_{\text{in}}} \| F(f_{\text{in}}, V) - F_\infty(f_{\text{in}}) \|^2,
	\quad V\in\mathbb V
	.
\end{equation*}
Here $F_\infty:\mathcal H_{\text{in}}\rightarrow\mathcal H_{\text{out}}$ is the ``reference function'',
that is a function that returns the ``reference output'' for each input (in principle we could have absorbed $F_\infty$ into $F$,
but for clarity we keep them separated).


\subsection{Gradient descent}

\begin{theorem}
	For $U\subset\mathcal W$ compact, and $w\in U$ fixed,
	there exists an $\epsilon>0$ such that
	$$
	S(w-\epsilon\nabla S(w)) \le S(w)
	.
	$$
	and if $\nabla S(w)\ne 0$, then $\le$ can be replaced by $<$.
\end{theorem}
\begin{proof}[Proof sketch]
For a fixed $V\in\mathbb V$, consider $\nabla S(V)$ as an element of $\mathbb V$.
Consider the expansion
$$
S(V-\epsilon\nabla S(V)) = S(V) - \epsilon {\|\nabla S(V)\|}^2 + R(V,\epsilon)
\quad V\in \mathcal U\subset\mathbb V,\quad 0<\epsilon<1
.
$$

Let
$\mathcal U \subset \mathbb V$, and $0<\epsilon<1$, such that  $R(w, \epsilon) \le C(U) \epsilon^2$, $w\in U$.
Then
\begin{align*}
	&S(V - \epsilon \nabla S(V) ) = S(V) - \epsilon {\| \nabla S(V) \|}^2  + \epsilon^2 C(\mathcal U)\\
	&S(V - \epsilon \nabla S(V) )  - S(V)  = -\epsilon {\| \nabla S(V) \|}^2+ \epsilon^2 C(\mathcal U)\\
	&\qquad \le -\epsilon  {\| \nabla S(V) \|}^2 + \epsilon^2 |C(\mathcal U)|
	.
\end{align*}
Assume $\|\nabla S(V)\|_{\mathbb V}\| \ne 0$, then
let $\epsilon$ such that $\epsilon|C(\mathcal U)| < \frac12 {\|\nabla S(w)\|}_{\mathcal W}$.
Then 
\begin{equation*}
	S(V - \epsilon \nabla S(V) )  - S(V) < -\frac{\epsilon}{2} {\|\nabla S(V)\|}_{\mathbb V} < 0
		.
\end{equation*}
 This concludes the proof.
\end{proof}


\subsection{MLP and filtration}
\label{sec:mlp-filtration}

% \tikzset{%
%    neuron missing/.style={
%     draw=none, 
%     scale=4,
%     text height=0.333cm,
%     execute at begin node=\color{black}$\vdots$
%   },
% }

% \begin{center}
%   \begin{tikzpicture}[x=1.5cm, y=1.5cm, >=stealth]

%     \foreach \m/\l [count=\y] in {1,2}
%     {
%       \node [circle,fill=green!50,minimum size=1cm] (input-\m) at (0,2.5-\y) {};
%     }
%     \foreach \m/\l [count=\y] in {3}
%     {
%       \node [circle,fill=green!50,minimum size=1cm ] (input-\m) at (0,-1.5) {};
%     }
%     \foreach \m/\l [count=\y] in {4}
%     {
%       \node [circle,fill=green!50,minimum size=1cm ] (input-\m) at (0,-3.5) {};
%     }
    
%     \node [neuron missing]  at (0,-2.5) {};

%     \node [neuron missing]  at (0,-0.5) {};
    

%     \foreach \m [count=\y] in {1}
%     \node [circle,fill=red!50,minimum size=1cm ] (hidden-\m) at (2,2.5-\y) {};

%     \foreach \m [count=\y] in {2}
%     \node [circle,fill=red!50,minimum size=1cm ] (hidden-\m) at (2,-0.5) {};
    
%     \foreach \m [count=\y] in {3}
%     \node [circle,fill=red!50,minimum size=1cm ] (hidden-\m) at (2,-3.5) {};
    
%     \node [neuron missing]  at (2,0.7) {};

%     \node [neuron missing]  at (2,-1.8) {};


%     \foreach \m [count=\y] in {1}
%     \node [circle,fill=blue!50,minimum size=1cm ] (output-\m) at (4,1.5-\y) {};
    
%     \foreach \m [count=\y] in {2}
%     \node [circle,fill=blue!50,minimum size=1cm ] (output-\m) at (4,-0.5-\y) {};

%     \node [neuron missing]  at (4,-0.4) {};

%     \foreach \l [count=\i] in {1,2,256}
%     \draw [<-] (input-\i) -- ++(-1,0)
%     node [above, midway] {$I_{\l}$};

%     \foreach \l [count=\i] in {1,16}
%     \node [above] at (hidden-\i.north) {$H_{\l}$};

%     \foreach \l [count=\i] in {1,16}
%     \draw [->] (output-\i) -- ++(1,0)
%     node [above, midway] {$O_{ \l}$};

%     \foreach \i in {1,...,3}
%     \foreach \j in {1,...,2}
%     \draw [->] (input-\i) -- (hidden-\j);

%     \foreach \i in {1,...,2}
%     \foreach \j in {1,...,2}
%     \draw [->] (hidden-\i) -- (output-\j);

%     % \foreach \l [count=\x from 0] in {Input, Hidden, Ouput}
%     % \node [align=center, above] at (\x*2,2) {\l \\ layer};

%   \end{tikzpicture}
% \end{center}


  We consider a MLP  with $N\in\NN$ neurons per layer and $L\in\NN$ layers.
  Let, with reference to  the notation of subsection~\ref{sec:general-notation},
  $\mathcal H^{\text{in}} \deq \RR^N$, $\mathbb B \deq \RR^{N}\times\RR^{L}$, $\mathbb W \deq \RR^{N}\times\RR^N\times\RR^L$.
  Moreover we consider each layer as being the input of the next.
  Therefore we set $\mathcal H$ to be a copy of $\mathcal H^{\text{in}}$
  as the abstract space of inputs to a generic layer $\ell$, with $\ell\in\{0,\dots, L\}$.
  Here the layer with $\ell=0$ represents the input layer, whereas the layer with $\ell=L$ represents the output layer.
  So the MLP has actually $L+1$ layers and $L-1$ hidden layers. 

  We consider the following decomposition of the weight over the layers.
  By this we mean the following.
  First, we decompose $\mathbb V =\mathbb B\oplus\mathbb V$ as a direct sum of spaces indexed by the layers:
  \begin{equation}
    \label{eq:1}
    \mathbb V = \underbrace{\mathcal V \oplus\cdots\oplus \mathcal V}_{L \text{ times}} ,
    \quad
    \mathcal V = \mathcal B\oplus \mathcal W, \quad \mathcal B\cong\RR^N,\quad\mathcal W\cong\RR^N\times\RR^N
    ,
  \end{equation}
  Now we have two equivalent options to think of the weights in this decomposition.
  Consider a layer $\ell$. Then this layer will have depend on a set of inputs and a set of weights.
  The set of weights that contribute to the layer $\ell$, but no layer $\ell'$ with $\ell'<\ell$, is by definition isomorphic to $\mathcal V$.
  We can think of this set  as either a set of ``additional inputs'' for the layer $\ell$ or as a set describing the ``state'' of layer $\ell$.
  The first interpretation is in a sense reminiscence of the von Neumann architecture,
  where data$=$ inputs and instructions$=$weights have the same representation (on the memory);
  on the other hand the second interpretation is reminiscent of the Harvard architecture, where data are instructions have different representation.
  In~\cite{goodfellowDeepLearning2016} the second option is taken.
  To have some diversity, we take the first.
  Beside the philosophical aspect, the only effective difference is wether we count the weights from $0$ to $L-1$ or from $1$ to $L$
  (cf.~\eqref{eq:2} below).
  Therefore, we order the $\mathcal V$ in the decomposition~\eqref{eq:1} starting from $0$ and ending with $L-1$.
  The $\ell$-th $\mathcal V$ in the decomposition will be thought of as an additional input to the layer $\ell+1$.

  The decomposition~\eqref{eq:1} gives rise to a ``filtration'' of spaces of weights.
  We define the spaces:
  \begin{align*}
    \mathbb V_{\llbracket \ell +h,\ell\rrbracket} &\deq \bigoplus_{\ell=\ell}^{\ell+h} \mathcal V = \underbrace{\mathcal V\oplus\cdots\oplus\mathcal V}_{h \text{ times}}
                                                    ,  \quad h\in\{0,\dots,L-1 -\ell\} ,  \quad\ell\in\{0,\dots,L-1\} ,\\
    \mathbb V_{\ell}&\deq \mathbb V_{\llbracket \ell,\ell \rrbracket} = \mathcal V ,\quad \ell\in\{0,\dots,L-1\},
    .
  \end{align*}
  These spaces define a filtration,
  i.e. we have a sequence of spaces $\mathbb V_{\llbracket \ell ,0 \rrbracket}$, $\ell\in\{1,\dots,L\}$, which satisfy
  \begin{align*}
    \mathbb V 
    =\mathbb V_{\llbracket L -1,0 \rrbracket}\supset 
     \cdots\supset \mathbb V_{\llbracket 1 ,0 \rrbracket} \supset\mathbb V_{\llbracket 0 ,0 \rrbracket}  \cong\mathcal V
    .
  \end{align*}
  We employ the same notation for the space $\mathbb B$, $\mathbb W$.
  The space $\mathbb V_{\llbracket \ell+h,\ell\rrbracket}$ represents the set of weight which ``connect'' the layer $\ell$ with the layer $\ell+h+1$.
  We think of an element $V\in\mathbb V$ as an $L$-tuple:
  \begin{equation*}
    V = (v^{L-1},v^{L-2},\dots,v^{0}), \quad v^\ell\in\mathcal V,\quad v^\ell=(b^\ell,w^\ell), \quad b^\ell\in\mathcal B, \quad w^\ell\in\mathcal W, \quad \ell\in\{0,\dots,L-1\}
    .
  \end{equation*}
  We denote the restriction of $V\in\mathbb V$ to $\mathbb V_{\llbracket \ell+h,\ell\rrbracket}$ by $V_{\llbracket\ell+h,\ell\rrbracket}$ with
  \begin{equation*}
    V_{\llbracket\ell+h,\ell\rrbracket} = (v_{\ell+h},v_{\ell+h-2},\dots,v_{\ell}), \quad v_\ell\in\mathcal V,\quad \ell\in\{\ell,\dots,\ell+h\}
    .
  \end{equation*}
  
  Let $\Phi^\ell:\mathcal H\rightarrow\mathcal H$ be a ``vector activation function''.
  To describe a simple MLP, we take $\Phi$ of the form
  \begin{align*}
    [\Phi^\ell( x )]_n = \varphi^\ell( x_n ), \quad x=(x_1,\dots,x_N)\in\mathcal H
    ,
  \end{align*}
  where $[\cdot]_n$, $n\in\{1,\dots,N\}$, denotes the $n$-th component of a vector in $\mathcal H$;
  here $\varphi^\ell:\RR\rightarrow\RR$ is the activation function of the $n$-th neuron on the $\ell$-th
  layer and every neuron on the same layer has the same activation function.
  
  We define recursively the MLP characteristic  function
  $F:\mathbb V\times \mathcal  H_{\text{in}}\rightarrow \mathcal H_{\text{out}}$:
  \begin{equation}
    \label{eq:2}
    \begin{aligned}
      F(V,f) &\deq x^{L} , \\
      x^{\ell+1} &\deq b^\ell + w^\ell \Phi^{\ell+1} ( x^\ell ),\quad \ell \in \{ 0,\dots , L-1\} , \\
      x^{0} &= f^{in},
    \end{aligned}
  \end{equation}
  where $v=(w,b)\in\mathcal V$.
  For example for $N=1$ and $L=3$ we have:
  $$
  F(f_{\text{in}},v) \deq  w^{(2)} \varphi( w^{(1)} \varphi( w^{(0)} f_{\text{in}} + b^{(0)} ) + b^{(1)} ) + b^{(2)} 
  .
  $$
  We see in~\eqref{eq:2}  that the weights can be thought of as either additional inputs or as state of a layer.
  In~\eqref{eq:2} the weights and the inputs appear on the right hand side with the same index $\ell$ and can be thoughts as data fed to the layer $\ell+1$.
  On the other hand we have $\Phi^{\ell+1}$ because we think of $\Phi^\ell$ as the activation function of the layer $\ell$.
  The more common (and perhaps sensible) convention (cf.~\cite{goodfellowDeepLearning2016}) is to replace the second line of~\eqref{eq:2} with
  $$
  x^{\ell} = b^{\ell} + w^{\ell} \Phi^\ell ( x^{\ell-1} ),\quad \ell \in \{ 1,\dots , L\}
  ,
  $$
  where for clarity we have translated the $\ell$ to $\ell+1$.
  Then the weights $(b^{\ell},w^{\ell})$ would be thought of as part of the function $X^{\ell}$ which sends $x^{\ell-1}:\mapsto x^{\ell}$
  (we will come back to this function in~\eqref{eq:3} below).
  
  We define the cost function to be $S:\mathbb V \rightarrow\RR$.
  $$
  S(v) \deq \sum_{\fin\in\mathcal H^{\text{in}}} \|F(f_{\text{in}},v) - F_{\infty}(f_{\text{in}} ) \|^2
  .
  $$

  We want to compute the gradient of $S(V) $.
  We want to do this rigorously and explicitly so that the implementation of the algorithm becomes straightforward.
  In~\eqref{eq:2} each $x^{\ell}$ is an element of $\mathcal H$  and not a function.
  To take derivative we need a notation which expresses the dependence of the $x^{\ell}$, $\ell\in\{0,\dots,L\}$ on the weights (and on each other).
  We do this taking advantage of the ``filtration'', i.e. on the fact that each layer can be thought as having a transition function
  which takes as inputs the outputs of the previous layer.
  
  To express this concretely we define the $\ell$-th ``\textit{layer transition function}'':
  \begin{equation}\label{eq:3}
    X^{\ell+1}_{v}(x) \deq b^\ell + w^\ell\Phi^{\ell+1}(x),
                            \quad v=(b,w)\in\mathcal V,
                            \quad x\in\mathcal H
                            \quad \ell\in \{0,\dots, L-1\}
                            .
  \end{equation}
  We then define the ``\textit{filtered transition functions}'' to be the functions $F^{\ell',\ell}$, $\ell'>\ell$,  
  given by
  \begin{align*}
    F^{\ell+h+1,\ell}(V_{\llbracket \ell + h, \ell \rrbracket}, x) &\deq X^{\ell+h}_{v^{\ell+h-1}}  (\dots  X^{\ell+2}_{v^{\ell+1}} ( X^{\ell+1} _{v^{\ell}} (x) ) \dots ) \\
                                                           &\equiv (X^{\ell+h}_{v^{\ell+h-1}} \circ \dots \circ X^{\ell+2}_{v^{\ell+1}}  \circ X^{\ell+1}_{v^{\ell}}  )(x),
                                                             \quad \quad h\in\{ 0,\dots, L-1-\ell   \} , \quad \ell\in\{0,\dots,L-1\}
                         ,
  \end{align*}
  where $\circ$ denotes composition of functions.
  By definition we have  
  \begin{equation}
    \label{eq:4}
    \begin{aligned}
      F^{L,0} (V,\fin) =       F(V,\fin) , \quad      F^{\ell+1,\ell} \equiv X^{\ell+1}
                                                                     ,
        \quad \ell\in\{0,\dots,L-1\}
        .
    \end{aligned}
  \end{equation}
  Moreover the filtered transition functions satisfy the composition rule
  \begin{equation}
    \label{eq:5}
    F^{L,0}(V,\fin) = F^{L,\ell}(V_{\llbracket L-1,\ell\rrbracket} , F^{\ell,0}( V_{\llbracket \ell-1,0\rrbracket},\fin)  )
    .
  \end{equation}
  Finally consider the set of variables $(x^\ell)_{\ell\in\{0,\dots,L\}}$ defined in~\eqref{eq:2}.
  Each $x^\ell+1$, $\ell\in\{0,\dots,L-1\}$, is  the output after the first $\ell$ layers have been applied to the inputs, that is, we have
  \begin{equation}
    \label{eq:6}
    x^{\ell+1} = F^{\ell+1,0}(V_{\llbracket \ell, 0\rrbracket} , \fin) ,\quad \ell\in\{0,\dots,L-1\}
    .
  \end{equation}

  With this notation, we can rewrite~\eqref{eq:2} in the following way:
  \begin{equation}
    \label{eq:7}
    \begin{aligned}
      F(V,f^{\text{in}}) &= F^{L,0}(V, f^{\text{in}})  , \\
      F^{\ell+1,0}( V_{\llbracket \ell,0\rrbracket} , f^{\text{in}}  )
                         &= b^\ell + w^\ell \phi^{\ell+1}( F^{\ell,0}( V_{\llbracket \ell-1,0\rrbracket} , f^{\text{in}}) ) ,
                           \quad \ell\in\{0,\dots, L-1\} 
                           .
    \end{aligned}
  \end{equation}
  We further rewrite these relations in a way that makes the parallel with~\eqref{eq:2} obvious.
  Note first that
  \begin{align*}
    F^{\ell+1,0}( V_{\llbracket \ell ,0\rrbracket}, \fin )
    &= X^{\ell+1}_{v^\ell} ( X^\ell_{v^{\ell-1}} ( x^{\ell-1} ) ), \\
    F^{\ell,0}(  V_{\llbracket \ell-1 ,0\rrbracket}, \fin )
    &= X^{\ell}_{v^{\ell-1}}( x^{\ell-1} ) 
        ,
  \end{align*}
  where the $(x^\ell)_{\ell\in\{0,\dots,L\}}$ are defined in~\eqref{eq:2} and satisfy~\eqref{eq:6}.
  Then we can rewrite~\eqref{eq:2} as follows
  \begin{equation}
    \label{eq:8}
    \begin{aligned}
      F(V,\fin) &=  X^{L}_{v^{L-1}}(  x^{L-1} ), \\
      X^{\ell+1}_{v^\ell}( X^\ell_{v^{\ell-1}} ( x^{\ell-1} ) )
                         &=  b^\ell + w^\ell  \Phi^{\ell+1} ( X^{\ell}_{v^{\ell-1}} ( x^{\ell-1} ) ),
                           \quad \ell\in\{0,\dots, L\} 
                           .
    \end{aligned}
  \end{equation}
  Comparing~\eqref{eq:8} with~\eqref{eq:2}, we see that we have the same form or inductive relation but in~\eqref{eq:8} the symbols
  $X^{\ell}$ are now functions.

  Strong of this (perhaps a bit over complicated..) notation we are ready to compute the gradient $\nabla S$.

  \subsection{Back-propagation}
  For convenience we call \textit{gradient} both the gradient of a scalar-valued function and the Jacobian of a vector valued function.
  We shall use the ``chain-rule'' for the gradient in the following form
  (cf.~ \cite[formula (1.4) p. 23, and Theorem (2.3) p. 27]{boothbyIntroductionDifferentiableManifolds2003}).
  Given two functions $F:\RR^\mu\rightarrow\RR^\nu$, $G:\RR^\nu\rightarrow \RR^\rho $, $\mu,\nu,\rho\in\NN$, we let
  $H\deq G\circ F:\RR^\mu\rightarrow\RR^\rho$.
  Then we have
  \begin{equation}
    \label{eq:9}
    (DH)( a ) = (DG)( F(a) ) (DF)(a), \quad a\in\RR^\nu
    ,
  \end{equation}
  where $DF$ and $DG$ represent the gradients of $F$ and $G$ 
  and  where on the right-hand side we have the matrix product of the matrix $(DG)( F(a) )$ with the matrix $(DF)(a)$.
  Note that these two matrices have, in general, different shapes:
  $DG( F(a) )$ is a linear map $\RR^{\rho}\rightarrow\RR^{\nu}$, hence it is represented by a matrix of shape $(\rho,\nu)$, 
  whereas $DF(a)$ is a linear map $\RR^{\nu}\rightarrow\RR^{\mu}$, thus its matrix representation has shape $(\nu,\mu)$.
  
  We use the same convention as~\cite{boothbyIntroductionDifferentiableManifolds2003} with regard to the definition
  of the matrix $DF(a)$, that is
  \begin{equation*}
    [DF(a)]_{n_1 n_2} \deq \frac{\partial F_{n_1} (x)}{\partial x_{n_2}}\Big|_{x=a}
    .
  \end{equation*}

  
  Before computing the derivatives we introduce two notational conventions which will make the formulas clearer.

  First: Consider a filtered transition function $F^{\ell',\ell}:\mathbb V_{\llbracket \ell' , \ell \rrbracket} \times\mathcal H\rightarrow \mathcal H$,
  $\ell'>\ell$, $\ell',\ell\in\{0,\dots,L+1\}$.
%  We denote the gradient of $F^{\ell',\ell}$ by $\mathcal D F^{\ell',\ell}$.
  Since $F^{\ell',\ell}$ is a function of a pair of vector-variables $(V_{\llbracket \ell' -1, \ell \rrbracket},x)$,
  where
  $V_{\llbracket \ell'-1 , \ell \rrbracket}\in\mathbb V_{\llbracket \ell'-1 , \ell \rrbracket}$ are the weights, and
  $x\in\mathcal H$ are the inputs,
  we distinguish the gradients with respect to each of these variables.
  We denote the gradient with respect to the weights by $\nabla F^{\ell',\ell}$
  and we denote the gradient with respect  to the inputs by $\mathtt D F^{\ell',\ell}$.
  % Moreover given that a weight $v\in\mathcal V$ is itself a pair  $v=(b, w)$, $b\in\mathcal B$, $w\in\mathcal W$,
  % we shall distinguish between the gradient ``in the direction $\mathcal B$'', which we shall denote by $

  
  Second: In computing the gradient, we want to take advantage of the ``compositional nature'' of the transition function.
  To express this in the notation we define a gradient $\nabla^\ell$ to denote the gradient with respect to the variable
  $v^\ell$ which lives in the $\ell$-th component of the decomposition
  $\mathbb V=\underbrace{\mathcal V\oplus\cdots\oplus\mathcal V}_{L\text{ times}}$.

  % Third: Given two vectors $x,y\in\mathcal H$ we have the scalar product $\langle x, y\rangle\in\RR$.
  % We shall need an expression for pairing, instead of just two vectors, two tensors of different degree,
  % i.e. two arrays of different shape.
  % Let  $T\in\mathcal H^{\otimes d}$, respectively   $T'\in\mathcal H^{\otimes d'}$,
  % be a tensor of degree $d\in\NN$, respectively $d'\in\NN$.
  % This means that for example $T$ is represented by an array with $d$ indices:
  % $[T]_{n_1,\dots,n_d}$, $n_j\in\{1,\dots, N \}$ ($N$ being as before the dimension of $\mathcal H$).
  % We define the binary operation $\boldcol$  as follows.
  % It sends  $T,T'$ into the matrix $T\boldcol T'$ obtained by
  % contracting the right-most index of $T$ with the left-most index of $T'$.
  % That is $T\boldcol T'$ is a tensor of degree $d-1+d'-1 =d+d'-2$ of components:
  % \begin{align*}
  %   [ T\boldcol T' ]_{n_1,\dots,n_{d-1}, n'_2,\dots, n'_{d'}}
  %   &\deq \sum_{m=1}^N [T]_{n_1,\dots,n_{d-1},m} [T']_{m,n'_2,\dots,n'_{d'}}
  %   .
  % \end{align*}
  % For two vectors $x,y\in\mathcal H$ we have $x\boldcol y =\langle x,y\rangle$.
  % For two matrices $A,B\in\mathcal H\otimes\mathcal H$ we have $A\boldcol B = AB$
  % where $AB$ denotes the standard matrix product of $A$ and $B$.
  % As this last example shows, the binary operation is \textit{not} symmetric (i.e. commutative) in general.
    
  With all of this out of the way,  we compute step by step the derivatives:
  First we have:  
  \begin{align*}
    [\nabla^\ell S(v)]_m
    &= \frac{\partial}{\partial [v^\ell]_m} S(v) \\
    &= \sum_{\fin\in\mathcal H^{\text{in}}} \frac{\partial}{\partial [v^\ell]_m} \sum_n (F_n(V,\fin) - [F_\infty]_n  )^2 \\
    &= \sum_{\fin\in\mathcal H^{\text{in}}} \sum_n 2(F_n(V,\fin) - [F_\infty]_n)\frac{\partial}{\partial [v^\ell]_m} F_n(V,\fin)\\
    &= \sum_{\fin\in\mathcal H^{\text{in}}} \sum_n \big( \frac{\partial}{\partial [v^\ell]_m} F_n(V,\fin) \big) 2(F_n(V,\fin) - [F_\infty]_n)\\
    &= \sum_{\fin\in\mathcal H^{\text{in}}} \sum_n 2 [(\nabla^\ell F)(V,\fin)^{\mathtt t}]_{mn} (F_n(V,\fin) - [F_\infty]_n)
    ,
  \end{align*}
  where the superscript $\mathtt t$ denotes the matrix-transposition.
  We have therefore:
  \begin{equation}\label{eq:10}
    \nabla^\ell S(V)
    = \sum_{\fin\in\mathcal H^{\text{in}}}  2(\nabla^\ell F)(V)^{\mathtt t}   (F(V,\fin) - F_\infty)  ,
      \quad      V\in\mathbb V,
      \quad \ell\in\{0,\dots,L-1\}
      ,
  \end{equation}
  where on the right-hand side we have the standard ``row-column product'' of the matrix $2(\nabla^\ell F)(V)^{\mathtt t}$
  with the (column) vector $(F(V,\fin) - F_\infty)$.
  
  Second, we want to compute $\nabla^\ell F$.
  Because of the composition property~\eqref{eq:5} of the filtered transition functions, we can apply the chain rule~\eqref{eq:9}. 
  The straight forward specialization of that formula to our case reeds\footnote{This is a complicated way to write:
    \begin{align*}
      \frac{\partial F_k(v)}{\partial v^\ell} 
      &= \frac{\partial x^L}{\partial v^\ell} \\
      &= \frac{\partial x^L}{\partial x^{\ell+1} }\frac{\partial x^{\ell+1}}{\partial v^\ell}
        .
    \end{align*}
    In this way we reduce the problem of computing the gradient $\nabla F$ to the problem of
    computing the partial derivatives $\partial x^L / \partial x^{\ell+1}$.
    These derivatives can be computed recursively (applying again the chain rule). This is the origin of the name \textit{back-propagation}.
    Indeed,  we have
    \begin{equation}\label{eq:11}
      \begin{aligned}
        \frac{\partial x^{L+1}}{\partial x^{\ell+1}}
        &= \frac{\partial x^{L+1}}{\partial x^{\ell+2}}\frac{\partial x^{\ell+1}}{\partial x^{\ell+1}}
          .
      \end{aligned}
    \end{equation}
    We rewrite this in terms of the more rigorous notation employing the functions $F^{\ell',\ell}$.
 }:
  \begin{equation}\label{eq:12}
    \begin{aligned}
      \nabla^{\ell-1} F(V,\fin)
      &= \nabla^{\ell-1} F^{L,\ell}([V]_{L-1,\ell}, F^{\ell,0}([V]_{\ell-1,0} , \fin ) ) \\
      &= (\mathtt D F^{L,\ell})([V]_{L-1,\ell}, x^\ell ) (\nabla^{\ell-1}  F^{\ell,0})([V]_{\ell-1,0} , \fin )
        ,
    \end{aligned}
  \end{equation}
  where, as before, $x^{\ell} we . = F^{\ell,0}([V]_{\ell-1,0}, \fin )$.

  In formula~\eqref{eq:12} the term $ (\nabla^{\ell-1}  F^{\ell,0})([V]_{\ell-1,0} , \fin )$ can already be written down explicitly.
  To write it down we recall that the weight $v^\ell$ is composed of two parameters:  $v^\ell=(b^\ell,w^\ell)$.
  Hence we denote by $\nabla^\ell_b$, respectively $\nabla^\ell_w$, the gradient with respect to $b^\ell$, respectively $w^\ell$.
  We obtain, for $\ell\in\{1,\dots,L\}$,
  \begin{equation}\label{eq:13}
    \begin{aligned}
      (\nabla^{\ell-1}_b  F^{\ell,0})([V]_{\ell-1,0} , \fin ) &= \id_{\mathcal H\rightarrow\mathcal H} ,\\
      (\nabla^{\ell-1}_w  F^{\ell,0})([V]_{\ell-1,0} , \fin ) &= \Phi^{\ell-1}(  F^{\ell-1,0}([V]_{\ell-2,0}, \fin  )^{\mathcal H\leftarrow\mathcal W}
                                                                ,
    \end{aligned}
  \end{equation}
  where $\id_{\mathcal H\rightarrow\mathcal H}$ denotes the identity $N\times N$ matrix, that is the identity on $\mathcal H$,
  and $\Phi^{\ell-1}(  F^{\ell-1,0}([V]_{\ell-2,0}, \fin  )^{\mathcal H\leftarrow\mathcal W}$ denotes the map $\mathcal W\rightarrow\mathcal H$
  given in components by
  \begin{equation*}
    \Phi^{\ell-1}(  F^{\ell-1,0}([V]_{\ell-2,0}, \fin  )^{\mathcal H\leftarrow\mathcal W} ( u ) 
    =  u \, \Phi^{\ell-1}(  F^{\ell-1,0}([V]_{\ell-2,0}, \fin  ) ,
  \end{equation*}
  where on the right-hand side we consider the matrix product of the matrix $u\in\mathcal W$, thought of as a linear map $u:\mathcal H\rightarrow\mathcal H$,
  with the vector $\Phi^{\ell-1}(  F^{\ell-1,0}([V]_{\ell-2,0}, \fin  ) \in\mathcal H$.
  In components we have
  \begin{equation*}
    [    \Phi^{\ell-1}(  F^{\ell-1,0}([V]_{\ell-2,0}, \fin  )^{\mathcal H\leftarrow\mathcal W} ]_{m, n_1, n_2} = \delta_{m n_1}  [ \Phi^{\ell-1}(  F^{\ell-1,0}([V]_{\ell-2,0}, \fin  ) ]_{n_2}
    ,\quad m,  n_1, n_2\in\{ 1,\dots, N\}
    ,
  \end{equation*}
  where $m$ is  the index of a one dimensional array representing a vector $x\in\mathcal H$ whereas
  $n_1, n_2$ are indices of a two dimensional array representing an element $w\in\mathcal W$.
  At the moment the notation in~\eqref{eq:13} is the best I could come up with, but it doesn't look completely satisfactory.
  One should maybe first start by introducing a pairing between elements in $\mathcal W$ and elements in $\mathcal H$
  (the standard product of a matrix with a column vector) and then talk about the ``dual'' with respect to such a pairing,
  but I'm still unsure.

  Third:
  The term $(D F^{L,\ell})[V_{\llbracket L-1,\ell\rrbracket}, x^\ell )$, on the right-hand side of~\eqref{eq:12},
  can be computed by applying again the chain rule~\eqref{eq:9}.
  In this way we get the following recursive relation which is the origin of the name \textit{back-propagation}:
  \begin{equation}\label{eq:14}
    \begin{aligned}
      (D F^{L,\ell-1})(V_{\llbracket L - 1 , \ell-1 \rrbracket } , x^{\ell-1}  )
      &= (D F^{L,\ell})(V_{\llbracket L-1-1,\ell \rrbracket}, x^{\ell} )  (DX^{\ell}_{v^{\ell-1}}) ( x^{\ell-1} )   
        ,
    \end{aligned}
  \end{equation}
  where  we have used the fact that
  \begin{equation*}
    x^{\ell} = X^{\ell}_{v^{\ell-1}} ( x^{\ell-1} ),
  \end{equation*}
  and where, as before, on the right-hand side of~\eqref{eq:14}, we have the denoted matrix product of the two Jacobian matrices  simply by juxtaposition.
  Note that the Jacobian matrix $DX^{\ell-1}_{v^{\ell-2}}( x^{\ell-2 } )$ in~\eqref{eq:14} can be computed explicitly.
  We get, by linearity of the gradient,
  \begin{align*}
    [(DX^{\ell-1}_{v^{\ell-2}}) ( x^{\ell-2} ) )]_{mn}
    &=  \frac{\partial}{\partial x_n} \big( b^{\ell-2}_m + \sum_k w^{\ell-2}_{mk}\Phi^{\ell-1}_k( x ) \big)\Big|_{x = x^{\ell-2}} \\
    &=   \sum_k w^{\ell-2}_{mk} \frac{\partial}{\partial x_n} \Phi^{\ell-1}_k( x )  \Big|_{x = x^{\ell-2}} \\
    &=   \sum_k w^{\ell-2}_{mk}  [D\Phi^{\ell-1}( x^{\ell-2} )]_{kn}  \\
    &=    [ w^{\ell-2} \,  (D\Phi^{\ell-1})( x^{\ell-2} )  ]_{mn} 
    .
  \end{align*}
  Note that 
  \begin{equation}
    \label{eq:15}
    \begin{aligned}
      [D\Phi^{\ell}(x^{\ell-1})]_{kn}
      &=  \frac{\partial}{\partial x_n} \varphi^\ell( x_k ) \Big|_{x = x^{\ell-1} } \\
      &=  (\varphi^\ell)' ( x_k )  \frac{\partial x_k}{\partial x_n}\Big|_{x = x^{\ell-1}} \\
      &=  (\varphi^\ell)' ( x_k )  \delta_{nk} \Big|_{x = x^{\ell-1} } \\
      &= (\varphi^\ell)'( [x^{\ell-1}]_n) \delta_{nk} \;\text{ (no sum over repeated indices)}
        ,
    \end{aligned}
  \end{equation}
  where the prime ($'$) denotes the derivative of the function.
  Hence,~\eqref{eq:14} becomes:
  \begin{equation}\label{eq:16}
    \begin{aligned}
      (D F^{L,\ell-1})(V_{\llbracket L- 1 , \ell-1 \rrbracket } , x^{\ell-1} )
      &= (D F^{L,\ell})(V_{\llbracket L-1,\ell \rrbracket},  x^{\ell}  ) \, w^{\ell-1} \, (D\Phi^{\ell})(x^{\ell-1} )  
        .
    \end{aligned}
  \end{equation}

  
  To obtain this formula we had to express each object as a function, so that we could differentiate with respect to its argument.
  Now, to clarify the implementation of this formula, we go back to a ``declarative'' notation as in~\eqref{eq:11}.
  We fix the weights $V\in\mathbb V$ and the inputs $\fin\in\mathcal H^{\text{in}}$, then we let
  \begin{equation}\label{eq:17}
    \mathbf J^{\ell} := (D F^{L,\ell})(V_{\llbracket L - 1 , \ell  \rrbracket } , x^{\ell} ) )
    ,
    \quad\ell\in\{0,\dots,L-1\}
    ,
  \end{equation}
  where as usual the $x^{\ell}$ are the constant vectors of~\eqref{eq:2} and~\eqref{eq:6}. 
  Hence, from formula~\eqref{eq:16}, we get the ``back-propagating'' recursive relation
  \begin{equation}\label{eq:18}
    \mathbf J^{\ell-1}  =   \mathbf J^{\ell}  \, w^{\ell-1} \,(\mathtt D\Phi^{\ell})(x^{\ell-1}),
    \quad \ell\in\{1,\dots,L+1\}
    .
  \end{equation}
  This formula is ``backward-propagating'' in the sense that to compute the gradient of the function $F^{\text{out},\ell+1}$
  which takes as input the outputs of the layer $\ell$ we use the gradient of the function $F^{\text{out},\ell+2}$
  which takes as inputs the outputs of the following layer $\ell+1$, this means that we are ``propagating backward''
  from the layer $\ell+1$ to the layer $\ell$.
  
  We put everything together in a formula ready for implementation.
  We revert to the common convention where the weights for a given layer are thought of as describing a ``state'' of that layer
  instead of being thought of as an additional set of inputs for such a layer.
  \begin{proposition}
    Fix $M,L\in\NN$.
    Let, as in section~\ref{sec:mlp-filtration}, the input space $\mathcal H$ and the weight space $\mathbb V$ be defined as follows:
    \begin{align*}
      \mathcal H &\deq \RR^N,\\
      \mathbb V &\deq \mathbb B \oplus \mathbb W = \oplus_{\ell=0}^L\mathcal V,
                  \quad \mathbb B \deq \oplus_{\ell=0}^L \mathcal B \cong \RR^N\times \RR^L ,
                  \quad  \mathbb W \deq \oplus_{\ell=0}^L \mathcal W \cong \RR^N\times\RR^N\times \RR^L\\
      \mathcal V &\deq \mathcal B \oplus\mathcal W, \quad \mathcal B \deq \RR^N , \quad   \mathcal W \deq \RR^N\times\RR^N
                  .
    \end{align*}
    For $\ell\in\{1,\dots,L\}$, we denote by $\varphi^\ell:\RR\rightarrow\RR$ be the activation function
    of a neuron on the $\ell$-th layer and we define the functions $\Phi^\ell:\mathcal H\rightarrow\mathcal H$ by
    \begin{align*}
      [\Phi^\ell( x )]_n = \varphi ([x]_n) , \quad x\in\mathcal H
      ,
    \end{align*}
    where $[y]_n$ denotes the $n$-th component of a vector $y\in\mathcal H$.
    Let the characteristic  function
    $F:\mathbb V\times \mathcal  H \rightarrow \mathcal H$
    be defined as follows.
    Fix a given input $f\in\mathcal H$ and a collection of weights $V\in\mathbb V$,
    $V = (v^0,\dots,v^L)$, where,
    for $\ell\in\{0,\dots,L\}$,
    $v^\ell = (b^\ell,w^\ell)$, $b^\ell\in\mathcal B$, $w^\ell\in\mathcal W$.
    Then, we define recursively a collection of ``partial inputs''
    $(x^\ell)_{\ell\in\{0,\dots,L+1\}}$, $x^\ell\in\mathcal H$:
    \begin{equation}  \label{eq:19}
      \begin{aligned}
        x^{\ell} &\deq b^\ell + w^\ell \Phi^\ell ( x^{\ell-1} ),\quad \ell \in \{ 1,\dots , L\} , \\
        x^{0} &= f
                .
      \end{aligned}
    \end{equation}
    Finally, we set (cf.~\eqref{eq:11}) $ F(V,f) = x^{L}$.
    Let the cost function $S:\mathbb V \rightarrow\RR$ be:
    \begin{equation*}
      S(V) \deq \sum_{f\in\mathcal H} \|F(f,V) - F_{\infty}(f ) \|^2
      .
    \end{equation*}
    
    Then the collection of gradients $\nabla S(V)$ is obtained via the following relations,
    with $\ell\in\{1,\dots,L\}$,
    \begin{equation}
      \label{eq:20}
      \begin{aligned}
        [\mathtt D\Phi^\ell(x^{\ell-1})]_{mn} &= \varphi'( [a]_n) \delta_{mn} \; \text{ (no sum over repeated indices) } ,\\
        \mathbf{J}^{\ell-1} &= \prod_{\ell'=L}^{\ell} \left(   w^\ell \,  (\mathtt D \Phi^{\ell'})  (x^{\ell'-1}) \right)
                             =   \mathbf J^{\ell} \, w^{\ell} \, (\mathtt D\Phi^{\ell}) (x^{\ell-1})  ,
                             \quad \mathbf J^L =\id  , \\
        \nabla^{\ell}_b F(V,f) &= \mathbf J^{\ell}  ,\\
        \nabla^{\ell}_w F(V,f) &= \mathbf J^{\ell} \Phi^{\ell}( x^{\ell-1} ) ,\\ 
        \nabla S(V) &= 2 (\nabla F)(V,f)^{\mathtt t} \,  (F(V,f) - F_\infty(f) )
                      .
      \end{aligned}
    \end{equation}
  \end{proposition}
  \begin{proof}
    We point to where the relations in~\eqref{eq:20} were obtained by ``back-propagating'' in this section (:\string^).
    The first line was obtained in~\eqref{eq:15};
    the second line is consequence of~\eqref{eq:16}, \eqref{eq:17}, \eqref{eq:18};
    the third and fourth lines are a consequence of~\eqref{eq:12} and~\eqref{eq:13};
    the forth line was obtained in~\eqref{eq:10}
    .
  \end{proof}
  \begin{corollary}
    With the same notation as the theorem, we have the following.
    \begin{align*}
      \nabla^{\ell-1}_b S( V )
      &= (\mathtt D \Phi^\ell)(x^{\ell-1} )^{\mathtt t} (w^\ell)^{\mathtt t} \nabla^{\ell}_b S( V ) , \\
      \nabla^{\ell-1}_w S( V )  
      &=2 ( \Phi^{\ell} (x^{\ell-1})^{\mathcal H\leftarrow\mathcal W} )^{\mathtt t}   \, (\nabla^{\ell}_b S)( V )
                                 ,
    \end{align*}
    where the transposition in the last line in components reads
    \begin{equation*}
      [ (\Phi^{\ell}(x^{\ell-1}) ^{\mathcal H\leftarrow\mathcal W})^{\mathtt t} ]_{n_1,n_2,m} = [\Phi^{\ell}(x^{\ell-1}) ^{\mathcal H\leftarrow\mathcal W}]_{m,n_1,n_2}
      ,
      \quad m,n_1,n_2\in\{1,\dots,N\}
      .
    \end{equation*}
  \end{corollary}
  \begin{proof}
    We have
    \begin{align*}
      \nabla^{\ell-1}_b S( V ) &= 2 (\mathbf J^{\ell-1})^{\mathtt t}  ( F(V,f) - F_\infty(f) )\\
                               &= (\mathtt D \Phi^\ell)(x^{\ell-1} )^{\mathtt t} (w^\ell)^{\mathtt t} \mathbf J^\ell (F(V,f) - F_\infty(f) ) \\
                               &= (\mathtt D \Phi^\ell)(x^{\ell-1} )^{\mathtt t} (w^\ell)^{\mathtt t} \nabla^{\ell}_b S( V ) , \\
      \nabla^{\ell-1}_w S( V ) &= 2  (\Phi^{\ell}(x^{\ell-1}) ^{\mathcal H\leftarrow\mathcal W})^{\mathtt t}
                                 \Phi^{\ell}(x^{\ell-1}) ^{\mathtt t}  (\mathbf J^{\ell-1})^{\mathtt t}  ( F(V,f) - F_\infty(f) )\\
                               &=2 (\Phi^{\ell}(x^{\ell-1}) ^{\mathcal H\leftarrow\mathcal W})^{\mathtt t} (\mathtt D \Phi^\ell)(x^{\ell-1} )^{\mathtt t} (w^\ell)^{\mathtt t} \nabla^{\ell}_b S( V )
                                 .
                                 \qedhere
    \end{align*}
  \end{proof}
% \newpage
% \subsection*{}

% \newpage

%\nocite{*}
%\bibliography{aipsamp}% Produces the bibliography via BibTeX.
%\bibliographystyle{plain}%  
\bibliographystyle{amsalpha}%   
\bibliography{ML}% 
     
\end{document}


%%% Local Variables:
%%% mode: latex
%%% TeX-master: t
%%% End:



